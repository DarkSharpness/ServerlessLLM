services:
  # Ray Head Node
  ray_head:
    build:
      context: ../../
      dockerfile: Dockerfile  # Ensure this points to your head node Dockerfile
    image: serverlessllm/sllm-serve
    container_name: ray_head
    environment:
      - MODEL_FOLDER=${MODEL_FOLDER}
    ports:
      - "6379:6379"    # Redis port
      - "8343:8343"    # ServerlessLLM port
    networks:
      - sllm_network
    command: >
      sh -c "/opt/conda/bin/sllm-serve start"

  # Ray Worker Node 0
  ray_worker_0:
    build:
      context: ../../
      dockerfile: Dockerfile.worker  # Ensure this points to your worker Dockerfile
    image: serverlessllm/sllm-serve-worker
    container_name: ray_worker_0
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: ["gpu"]
              count: 1  # Assigns 1 GPU to the worker
    environment:
      - WORKER_ID=0
      - MODEL_FOLDER=${MODEL_FOLDER}
    networks:
      - sllm_network
    volumes:
      - ${MODEL_FOLDER}:/models
    command: >
      sh -c "ray start --address='ray_head:6379' --num-gpus=1"

  # Ray Worker Node 1
  ray_worker_1:
    build:
      context: ../../
      dockerfile: Dockerfile.worker
    image: serverlessllm/sllm-serve-worker
    container_name: ray_worker_1
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: ["gpu"]
              count: 1  # Assigns 1 GPU to the worker
    environment:
      - WORKER_ID=1
      - MODEL_FOLDER=${MODEL_FOLDER}
    networks:
      - sllm_network
    volumes:
      - ${MODEL_FOLDER}:/models
    command: >
      sh -c "ray start --address='ray_head:6379' --num-gpus=1"

networks:
  sllm_network:
    driver: bridge
    name: sllm
